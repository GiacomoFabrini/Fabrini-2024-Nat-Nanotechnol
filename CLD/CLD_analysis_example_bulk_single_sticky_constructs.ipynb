{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7ff373",
   "metadata": {},
   "source": [
    "Data needed in input:\n",
    "- Chords extracted from segmentation masks of epifluorescence timelapses (.npy archives, here generally called `cld_results.npy`) in `cld_results_dir`. To obtain these: \n",
    "    - First get segmentation masks from epifluorescence timelapses by using the FIJI macro \"Fiji utils/RNA_epifluorescence_timelapse_segmentation.ijm\"). \n",
    "    - Then, perform CLD extraction using the notebook at \"CLD/CLD_from_Binary_Masks.ipynb\"\n",
    "- Particle Analysis results from binary segmentation (obtained from the first sub-step mentioned above) in `segmentation_results_dir`.\n",
    "\n",
    "This analysis applies both to experiments in bulk and within synthetic cells, as well as to single sticky constructs or binary systems with all sticky or sticky/non-sticky components. This script gives an example of this analysis applied to bulk assembly of single sticky constructs (A, B, C) (segmentation performed with Li thresholding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107871d",
   "metadata": {},
   "source": [
    "# Mean CLD and condensate number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process chord lengths while keeping information for different sample repeats\n",
    "# (i.e. microscopy FOVs) separate\n",
    "def cld_to_chords_unj(cld_dict, binning = 1): \n",
    "    \"\"\"\n",
    "    params: \n",
    "    cld_dict: input dictionary containing chords_x and chords_y per sample repeat vs time\n",
    "    binning: if images (and masks) have been binned, change binning factor, default is 1 \n",
    "    (image analysed in original format and resolution, 2044px x 2048px)\n",
    "    \n",
    "    returns: chords_vs_time_unj: dictionary containing merged XY chord lengths vs time \n",
    "        for each repeat (i.e. microscopy field-of-view) of each sample\n",
    "    \"\"\"\n",
    "    # Initialise dictionary\n",
    "    chords_vs_time_unj = {}\n",
    "    # Chord Lengths are in px -> need to convert to um to extract physical size information\n",
    "    px_um_conv = 3.0852 # px/um for 20x lens used on Nikon Ti2 -- CHANGE TO MATCH YOUR SETUP\n",
    "    # If we have binning, we need to adjust this px_um_conv factor\n",
    "    pxum_conv_bin = px_um_conv/binning \n",
    "    # Looping through samples in experiment\n",
    "    for ind, sample in enumerate(cld_dict.keys()): \n",
    "        print(sample)\n",
    "        # Initialising inner sample list\n",
    "        chords_vs_time_unj[sample] = {}\n",
    "        # Looping through repeats - keeping repeats separate\n",
    "        if len(list(cld_dict[sample].keys())) == 3: \n",
    "            for repeat in cld_dict[sample].keys(): \n",
    "                chords_vs_time_unj[sample][repeat] = []\n",
    "                # Looping through timepoints\n",
    "                for timepoint in tqdm(range(len(cld_dict[sample][1]['count_x']))):\n",
    "                    chords_xy = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat]['count_x'][timepoint],\n",
    "                         cld_dict[sample][repeat]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_vs_time_unj[sample][repeat].append(list(chords_xy))\n",
    "        # ...unless they refer to the same FOV, but different channels -- i.e. RNA nanostar C\n",
    "        elif len(list(cld_dict[sample].keys())) == 6: \n",
    "            print('More than repeats - will merge 1-4, 2-5, 3-6')\n",
    "            for repeat in list(cld_dict[sample].keys())[:3]: \n",
    "                chords_vs_time_unj[sample][repeat] = []\n",
    "                # Looping through timepoints\n",
    "                for timepoint in tqdm(range(len(cld_dict[sample][1]['count_x']))):\n",
    "                    chords_xy1 = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat]['count_x'][timepoint], \n",
    "                         cld_dict[sample][repeat]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_xy2 = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat+3]['count_x'][timepoint], \n",
    "                         cld_dict[sample][repeat+3]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_vs_time_unj[sample][repeat].append(list(chords_xy1)+list(chords_xy2))\n",
    "    return chords_vs_time_unj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ae008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute mean number of condensates in a given FOV \n",
    "# and the corresponding standard error of the mean\n",
    "def get_mean_stderr_number(df_list): \n",
    "    \"\"\"\n",
    "    params:\n",
    "    df_list: list of pandas dataframes from CSVs of Particle Analysis results performed in ImageJ/FIJI.\n",
    "    \n",
    "    returns: \n",
    "    mean, standard error of the mean of number of condensates per FOV\n",
    "    \"\"\"\n",
    "    num_list, lens = [], []\n",
    "    for df in df_list: \n",
    "        num_list.append(np.array(df.groupby('Slice').count()['Area']))\n",
    "        lens.append(len(np.array(df.groupby('Slice').count()['Area'])))\n",
    "    for i in range(len(num_list)): \n",
    "        if len(num_list[i]) < np.max(np.array(lens)): \n",
    "            # Pads with zeros in early timepoints in case of no condensates detected to ensure same length\n",
    "            num_list[i] = np.concatenate((np.zeros(np.max(np.array(lens) - len(num_list[i]))), num_list[i]))\n",
    "    num_list = np.array(num_list)\n",
    "    return np.mean(num_list, axis = 0), np.std(num_list, ddof = 1, axis = 0)/(len(num_list)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE TO ABSOLUTE PATHS TO CORRECT DIRECTORIES\n",
    "cld_results_dir = \"/ABSOLUTE/PATH/TO/CLD/RESULTS/\"\n",
    "segmentation_results_dir = \"/ABSOLUTE/PATH/TO/SEGMENTATION/RESULTS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results and process them\n",
    "cld_single_bulk = np.load(cld_results_dir + 'cld_single_bulk.npy', allow_pickle = True).item()\n",
    "chords_single_bulk_unj = cld_to_chords_unj(cld_single_bulk, binning = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e31b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and standard deviation vs time for all sample repeats\n",
    "mean_single_bulk_unj, std_single_bulk_unj = {}, {}\n",
    "# Loop through samples in dictionary keys\n",
    "for sample in chords_single_bulk_unj.keys(): \n",
    "    # Initialise blank timepoint-spanning lists within the output dictionaries\n",
    "    mean_single_bulk_unj[sample], std_single_bulk_unj[sample] = {}, {}\n",
    "    # Loop through repeats\n",
    "    for repeat in tqdm(range(1, 1+len(chords_single_bulk_unj[sample].keys()))):\n",
    "        #print(repeat)\n",
    "        mean_single_bulk_unj[sample][repeat], std_single_bulk_unj[sample][repeat] = [], []\n",
    "        # Loop through timepoints - one CLD per sample per timepoint\n",
    "        for timepoint in tqdm(range(len(chords_single_bulk_unj[sample][repeat]))):\n",
    "            mean_single_bulk_unj[sample][repeat].append(np.mean(chords_single_bulk_unj[sample][repeat][timepoint]))\n",
    "            std_single_bulk_unj[sample][repeat].append(np.std(chords_single_bulk_unj[sample][repeat][timepoint], ddof = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51291420",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_single_bulk_comb, std_single_bulk_comb = {}, {}\n",
    "# Loop through samples in dictionary keys\n",
    "for sample in mean_single_bulk_unj.keys(): \n",
    "    # Initialise blank timepoint-spanning lists within the output dictionaries\n",
    "    means_list = [np.array(mean_single_bulk_unj[sample][repeat]) for repeat in list(mean_single_bulk_unj[sample].keys())]\n",
    "    mean_single_bulk_comb[sample] = np.mean(means_list, axis = 0)\n",
    "    std_single_bulk_comb[sample] = np.std(means_list, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Number of Condensates vs Time\n",
    "\n",
    "# Helper function to read corresponding CSVs\n",
    "def read_segmentation_csvs(directory, sample_tag): \n",
    "    return [\n",
    "        pd.read_csv(file) \n",
    "        for file in list(os.listdir(directory)) \n",
    "        if sample_tag in file\n",
    "    ]\n",
    "\n",
    "# A and B - not using C as it does not form discrete condensates\n",
    "segm_a_unj = read_segmentation_csvs(segmentation_results_dir, '_A_')\n",
    "segm_b_unj = read_segmentation_csvs(segmentation_results_dir, '_B_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65548f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number of condensates (mean, standard error)\n",
    "num_a = get_mean_stderr_number(segm_a_unj)\n",
    "num_b = get_mean_stderr_number(segm_b_unj)\n",
    "\n",
    "# and compose into dictionary\n",
    "num_bulk = {\n",
    "    'NS_A': num_a, \n",
    "    'NS_B': num_b, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c517112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise x array = time (minutes)\n",
    "time = np.concatenate((np.arange(0, 10*60 + 15, 15), np.arange(10*60+30, 10*60+15 + 38*60 + 30, 30))) # minutes\n",
    "\n",
    "# Initialise colour palette and plotting labels\n",
    "colours = {'NS_A' : 'orangered', 'NS_B' : 'cyan', 'NS_C' : 'gray'}\n",
    "labels = {'NS_A' : 'A', 'NS_B' : 'B', 'NS_C' : 'C'}\n",
    "\n",
    "plt.subplots(2, 1, figsize = (3, 5))\n",
    "plt.subplots_adjust(hspace= .05)\n",
    "\n",
    "# Mean CLD\n",
    "plt.subplot(211)\n",
    "for ind, sample in enumerate(mean_single_bulk_comb.keys()): \n",
    "    # Plot solid line for mean profile\n",
    "    plt.plot(\n",
    "        time/60, \n",
    "        np.array(mean_single_bulk_comb[sample]), \n",
    "        lw = 2.0, \n",
    "        label = labels[sample], \n",
    "        color = colours[sample]\n",
    "    )\n",
    "    # Plot shaded region for standard error proxy = standard deviation at that particular timepoint\n",
    "    plt.fill_between(\n",
    "        time/60, \n",
    "        np.array(mean_single_bulk_comb[sample]) - np.array(std_single_bulk_comb[sample]), \n",
    "        np.array(mean_single_bulk_comb[sample]) + np.array(std_single_bulk_comb[sample]), \n",
    "        color = colours[sample], \n",
    "        alpha = 0.2\n",
    "    )\n",
    "    # Plot embellishments\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(direction = 'in', length = 6)\n",
    "    plt.ylabel(r'$\\rm\\mu_{CLD}$ [$\\rm\\mu$m]', fontsize = 20)\n",
    "    plt.yticks([0, 25, 50, 75], [0, 25, 50, 75], fontsize = 20)\n",
    "    plt.ylim([-5, 85])\n",
    "    plt.xticks([0, 12, 24, 36, 48], [], fontsize = 20)\n",
    "plt.legend(frameon = False, fontsize = 20)\n",
    "\n",
    "# Number of condensates per FOV\n",
    "plt.subplot(212)\n",
    "for ind, sample in enumerate(num_bulk.keys()): \n",
    "    if sample != 'NS_C':\n",
    "        # Plot mean number of condensates\n",
    "        plt.plot(\n",
    "            time/60, \n",
    "            num_bulk[sample][0], \n",
    "            lw = 2.0, \n",
    "            label = labels[sample], \n",
    "            color = colours[sample]\n",
    "        )\n",
    "        # Plot shaded regions = standard error of the mean\n",
    "        plt.fill_between(\n",
    "            time/60, \n",
    "            num_bulk[sample][0] - num_bulk[sample][1], \n",
    "            num_bulk[sample][0] + num_bulk[sample][1], \n",
    "            color = colours[sample], \n",
    "            alpha = 0.2\n",
    "        )\n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(direction = 'in', length = 6, which = 'major')\n",
    "        ax.tick_params(direction = 'in', length = 3, which = 'minor')\n",
    "        plt.ylabel('N', fontsize = 20)\n",
    "        plt.yticks(\n",
    "            [0, 500, 1000, 1500, 2000, 2500], \n",
    "            [0, 500, 1000, 1500, 2000, 2500], \n",
    "            fontsize = 20\n",
    "        )\n",
    "        plt.ylim([20, 5000])\n",
    "        if ind == 2: \n",
    "            plt.xlabel('Time [h]', fontsize = 20)\n",
    "            plt.xticks(\n",
    "                [0, 12, 24, 36, 48], \n",
    "                [0, '', 24, '', 48], \n",
    "                fontsize = 20\n",
    "            )\n",
    "        else: \n",
    "            plt.xticks(\n",
    "                [0, 12, 24, 36, 48], \n",
    "                []\n",
    "            )\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Time [h]', fontsize = 20)\n",
    "\n",
    "plt.xticks([0, 12, 24, 36, 48], [0, '', 24, '', 48], fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519dbfd",
   "metadata": {},
   "source": [
    "# Ridge Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34368c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to process CLD results for Ridge Plots\n",
    "def mean_std_stderr_cl(cld_dict, px_um_conv = 3.0852, binning = 1): \n",
    "    \"\"\"\n",
    "    params: \n",
    "    cld_dict: input dictionary containing chords_x and chords_y per sample repeat vs time\n",
    "    \n",
    "    px_um_conv: pixel to micron conversion factor for 20x lens used on Nikon Ti2 -- CHANGE TO MATCH YOUR SETUP\n",
    "    \n",
    "    binning: if images (and masks) have been binned, change binning factor, default is 1 \n",
    "    (image analysed in original format and resolution, 2044px x 2048px)\n",
    "    \n",
    "    returns: \n",
    "    chords_vs_time: dictionary containing chord lengths merged across repeats \n",
    "    and XY chords vs time for each sample\n",
    "    \n",
    "    mean_std: dictionary containing mean, standard deviation and standard error of the mean CLD \n",
    "    \"\"\"\n",
    "    # Chord Lengths are in px -> need to convert to um to extract physical size information\n",
    "    # If we have binning, we need to adjust this px_um_conv factor\n",
    "    pxum_conv_bin = px_um_conv/binning \n",
    "    \n",
    "    # Step 1: Extracting chord lengths per sample by merging chords_x and chords_y for all repeats (divided on a time basis) \n",
    "    # into single chords_vs_time dictionary. chords_vs_time dict has samples as keys,\n",
    "    # and values are chords vs time (merged x, y and across all sample repeats = imaged FOVs)\n",
    "    # Initialise unified dictionary\n",
    "    chords_vs_time = {}\n",
    "    # Looping through samples in experiment\n",
    "    for sample, ind in zip(cld_dict.keys(), range(len(cld_dict.keys()))): \n",
    "        print(sample)\n",
    "        # Initialising inner sample list\n",
    "        chords_vs_time[sample] = []\n",
    "        # Looping through timepoints\n",
    "        for timepoint in tqdm(range(len(cld_dict[sample][1]['count_x']))):\n",
    "            chords_xy_time = []\n",
    "            # Looping through repeats (FOVs in same sample/capillary)\n",
    "            for repeat in cld_dict[sample].keys(): \n",
    "                # Merge xy chords for the particular repeat\n",
    "                chords_xy = (1/pxum_conv_bin)*np.concatenate((cld_dict[sample][repeat]['count_x'][timepoint], \n",
    "                                        cld_dict[sample][repeat]['count_y'][timepoint]))\n",
    "                # Append to the growing list across repeats for this particular timepoint\n",
    "                chords_xy_time = chords_xy_time + list(chords_xy)\n",
    "            chords_vs_time[sample].append(chords_xy_time)\n",
    "    \n",
    "    \n",
    "    # Initialise dictionary containing mean chord_length, stddev and stderr on chord_length\n",
    "    mean_std = {'mean' : {}, 'std' : {}, 'stderr' : {}}\n",
    "    for sample, ind in zip(chords_vs_time.keys(), tqdm(range(len(chords_vs_time.keys())))): \n",
    "        mean_std['mean'][sample] = []\n",
    "        mean_std['std'][sample] = []\n",
    "        mean_std['stderr'][sample] = []\n",
    "        for timepoint in tqdm(range(len(chords_vs_time[sample]))): \n",
    "            x = chords_vs_time[sample][timepoint]\n",
    "            counts, bins = np.histogram(x)\n",
    "            mids = 0.5*(bins[1:] + bins[:-1])\n",
    "            probs = counts / np.sum(counts)\n",
    "            mean = np.sum(probs * mids) \n",
    "            sd = np.sqrt(np.sum(probs * (mids - mean)**2))\n",
    "            mean_std['mean'][sample].append(mean)\n",
    "            mean_std['std'][sample].append(sd)\n",
    "            mean_std['stderr'][sample].append(sd/np.sqrt(len(x)))\n",
    "        mean_std['mean'][sample] = np.array(mean_std['mean'][sample])\n",
    "        mean_std['std'][sample] = np.array(mean_std['std'][sample])\n",
    "        mean_std['stderr'][sample] = np.array(mean_std['stderr'][sample])\n",
    "    return chords_vs_time, mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CLD results (already loaded above)\n",
    "chords_vs_time_single_bulk, mean_std_stderr_bulk = mean_std_stderr_cl(cld_single_bulk, binning = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfebda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get KDE plots for various timepoints\n",
    "def get_kdes(chords_vs_time_dict, sample, timepoints): \n",
    "    xs, ys = [], []\n",
    "    for timepoint in timepoints: \n",
    "        x, y = sns.kdeplot(np.array(chords_vs_time_dict[sample][timepoint])).lines[0].get_data();\n",
    "        plt.close();\n",
    "        xs.append(np.array(x))\n",
    "        ys.append(np.array(y))\n",
    "    return xs, ys\n",
    "\n",
    "# Common features of ridge plots for bulk sticky constructs\n",
    "timepoints = [1, 25, 44, 56, 68, 80, 92, 104, 116]\n",
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "hours = [0, 6, 12, 18, 24, 30, 36, 42, 48]\n",
    "texts = [str(hour) + ' h' for hour in hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A - Figure S17\n",
    "xs, ys = get_kdes(chords_vs_time_single_bulk, 'NS_A', timepoints)\n",
    "    \n",
    "plt.figure(figsize = (10, 6))\n",
    "for i in range(len(xs)): \n",
    "    plt.plot(\n",
    "        xs[i], \n",
    "        ys[i] + (len(xs)-i-1)*0.015, \n",
    "        color = 'k'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        xs[i], \n",
    "        (len(xs)-i-1)*0.015, ys[i] + (len(xs)-i-1)*0.015, \n",
    "        color = 'orangered', \n",
    "        alpha = alphas[i]\n",
    "    )\n",
    "    plt.axhline((len(xs)-i-1)*0.015, color = 'black')\n",
    "    if i!=0:\n",
    "        plt.text(230, (len(xs)-i-1)*0.0153, texts[i], fontsize = 20)\n",
    "    else: \n",
    "        plt.text(230, (len(xs)-i-1)*0.0153, '15 min', fontsize = 20)\n",
    "plt.ylim([0, 0.20])\n",
    "sns.despine(left = True)\n",
    "plt.yticks([]); \n",
    "plt.xticks(\n",
    "    [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250], \n",
    "    [0, '', 50, '', 100, '', 150, '', 200, '', 250], \n",
    "    fontsize = 20\n",
    ")\n",
    "plt.xlabel(r'Size [$\\rm\\mu$m]', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d760100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B - Figure S17\n",
    "xs, ys = get_kdes(chords_vs_time_single_bulk, 'NS_B', timepoints)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "for i in range(len(xs)): \n",
    "    plt.plot(\n",
    "        xs[i],\n",
    "        ys[i] + (len(xs)-i-1)*0.017, \n",
    "        color = 'k'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        xs[i], \n",
    "        (len(xs)-i-1)*0.017, ys[i] + (len(xs)-i-1)*0.017, \n",
    "        color = 'cyan', \n",
    "        alpha = alphas[i]\n",
    "    )\n",
    "    plt.axhline((len(xs)-i-1)*0.017, color = 'black')\n",
    "    if i!=0:\n",
    "        plt.text(215, (len(xs)-i-1)*0.0173, texts[i], fontsize = 20)\n",
    "    else: \n",
    "        plt.text(215, (len(xs)-i-1)*0.0173, '15 min', fontsize = 20)\n",
    "plt.ylim([0, 0.22])\n",
    "sns.despine(left = True)\n",
    "plt.yticks([]); \n",
    "plt.xticks(\n",
    "    [0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250], \n",
    "    [0, '', 50, '', 100, '', 150, '', 200, '', 250], \n",
    "    fontsize = 20\n",
    ")\n",
    "plt.xlabel(r'Size [$\\rm\\mu$m]', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C - Figure S17\n",
    "xs, ys = get_kdes(chords_vs_time_single_bulk, 'NS_C', timepoints)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "for i in range(len(xs)): \n",
    "    plt.plot(\n",
    "        xs[i], \n",
    "        ys[i] + (len(xs)-i-1)*0.015, \n",
    "        color = 'k'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        xs[i], \n",
    "        (len(xs)-i-1)*0.015, ys[i] + (len(xs)-i-1)*0.015, \n",
    "        color = 'gray', \n",
    "        alpha = alphas[i]\n",
    "    )\n",
    "    plt.axhline((len(xs)-i-1)*0.015, color = 'black')\n",
    "    if i!=0:\n",
    "        plt.text(430, (len(xs)-i-1)*0.0153, texts[i], fontsize = 20)\n",
    "    else: \n",
    "        plt.text(430, (len(xs)-i-1)*0.0153, '15 min', fontsize = 20)\n",
    "plt.ylim([0, 0.24])\n",
    "sns.despine(left = True)\n",
    "plt.yticks([]); \n",
    "plt.xticks(\n",
    "    [0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500], \n",
    "    [0, '', 100, '', 200, '', 300, '', 400, '', 500], \n",
    "    fontsize = 20\n",
    ")\n",
    "plt.xlabel(r'Size [$\\rm\\mu$m]', fontsize = 20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
