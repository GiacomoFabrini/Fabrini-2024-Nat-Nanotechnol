{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46ec6e6",
   "metadata": {},
   "source": [
    "Data needed in input:\n",
    "- Chords extracted from segmentation masks of epifluorescence timelapses (.npy archives, here called `cld_bulk_thermal_ramps.npy`) in `thermal_ramps`. To obtain these: \n",
    "    - First get segmentation masks from epifluorescence timelapses by using the FIJI macro \"Fiji utils/RNA_epifluorescence_timelapse_segmentation.ijm\"). \n",
    "    - Then, perform CLD extraction using the notebook at \"CLD/CLD_from_Binary_Masks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import porespy as ps\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage.measure import regionprops\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b42379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with absolute path to folder containing the .npy file of chord lengths\n",
    "# extracted from thermal ramps CLD extraction (using `CLD/CLD_from_Binary_Masks.ipynb`).\n",
    "# In this script, the resulting file is named `cld_bulk_thermal_ramps.npy'.\n",
    "thermal_ramps = \"/ABSOLUTE/PATH/TO/CHORDS/NPY/FILE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cld_to_chords_unj(cld_dict, binning = 1): \n",
    "    # cld_dict = input dictionary containing chords_x and chords_y per sample repeat vs time\n",
    "    # binning = if images (and masks) have been binned, change binning factor -- default is 1 \n",
    "    # (image analysed in original format and resolution, 2044px x 2048px)\n",
    "    # Initialise dictionary\n",
    "    chords_vs_time_unj = {}\n",
    "    # Chord Lengths are in px -> need to convert to um to extract physical size information\n",
    "    px_um_conv = 3.0852 # px/um for 20x lens used on Nikon Ti2\n",
    "    # If we have binning, we need to adjust this px_um_conv factor\n",
    "    pxum_conv_bin = px_um_conv/binning \n",
    "    # Looping through samples in experiment\n",
    "    for ind, sample in enumerate(cld_dict.keys()): \n",
    "        print(sample)\n",
    "        # Initialising inner sample list\n",
    "        chords_vs_time_unj[sample] = {}\n",
    "        # Looping through repeats - keeping repeats separate\n",
    "        if len(list(cld_dict[sample].keys())) == 3: \n",
    "            for repeat in cld_dict[sample].keys(): \n",
    "                chords_vs_time_unj[sample][repeat] = []\n",
    "                # Looping through timepoints\n",
    "                for timepoint in tqdm(range(len(cld_dict[sample][1]['count_x']))):\n",
    "                    chords_xy = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat]['count_x'][timepoint],\n",
    "                         cld_dict[sample][repeat]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_vs_time_unj[sample][repeat].append(list(chords_xy))\n",
    "        # ...unless they refer to the same FOV, but different channels -- i.e. RNA nanostar C\n",
    "        elif len(list(cld_dict[sample].keys())) == 6: \n",
    "            print('More than repeats - will merge 1-4, 2-5, 3-6')\n",
    "            for repeat in list(cld_dict[sample].keys())[:3]: \n",
    "                chords_vs_time_unj[sample][repeat] = []\n",
    "                # Looping through timepoints\n",
    "                for timepoint in tqdm(range(len(cld_dict[sample][1]['count_x']))):\n",
    "                    chords_xy1 = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat]['count_x'][timepoint], \n",
    "                         cld_dict[sample][repeat]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_xy2 = (1/pxum_conv_bin)*np.concatenate(\n",
    "                        (cld_dict[sample][repeat+3]['count_x'][timepoint], \n",
    "                         cld_dict[sample][repeat+3]['count_y'][timepoint])\n",
    "                    )\n",
    "                    chords_vs_time_unj[sample][repeat].append(list(chords_xy1)+list(chords_xy2))\n",
    "    return chords_vs_time_unj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "def sigmoidify(arr): \n",
    "    arr2 = arr.copy()\n",
    "    midpoint = (np.max(arr) + np.min(arr))/2\n",
    "    for i in range(len(arr2)): \n",
    "        if arr2[i] >= midpoint: \n",
    "            arr2[i] = 1\n",
    "        else:\n",
    "            arr2[i] = 0\n",
    "    return arr[0]*arr2\n",
    "\n",
    "def get_step(step_func): \n",
    "    for i in range(len(step_func)): \n",
    "        if step_func[i] > 0 and step_func[i+1] == 0: \n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de605269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to correct directory and load the serialised chord lengths\n",
    "os.chdir(thermal_ramps)\n",
    "cld_thermal_ramps = np.load('cld_bulk_thermal_ramps.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7ba8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess to obtain joined X and Y chord lengths for all repeats individually\n",
    "# In the case of sample C, for which the analysis is carried out in both MG and DFHBI\n",
    "# channels, the chords from matching channels of the same FOV are concatenated together\n",
    "chords_melt_bulk_unj = cld_to_chords_unj(cld_thermal_ramps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and standard deviation vs time for all sample repeats\n",
    "mean_melt_bulk_unj, std_melt_bulk_unj = {}, {}\n",
    "# Loop through samples in dictionary keys\n",
    "for sample in chords_melt_bulk_unj.keys(): \n",
    "    # Initialise blank timepoint-spanning lists within the output dictionaries\n",
    "    mean_melt_bulk_unj[sample], std_melt_bulk_unj[sample] = {}, {}\n",
    "    # Loop through repeats\n",
    "    for repeat in tqdm(range(1, 1+len(chords_melt_bulk_unj[sample].keys()))):\n",
    "        #print(repeat)\n",
    "        mean_melt_bulk_unj[sample][repeat], std_melt_bulk_unj[sample][repeat] = [], []\n",
    "        # Loop through timepoints - one CLD per sample per timepoint\n",
    "        for timepoint in range(len(chords_melt_bulk_unj[sample][repeat])):\n",
    "            mean_melt_bulk_unj[sample][repeat].append(np.mean(chords_melt_bulk_unj[sample][repeat][timepoint]))\n",
    "            std_melt_bulk_unj[sample][repeat].append(np.std(chords_melt_bulk_unj[sample][repeat][timepoint], ddof = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine information from all repeats to yield a single mean and standard error arrays per sample\n",
    "mean_melt_bulk_comb, std_melt_bulk_comb = {}, {}\n",
    "# Loop through samples in dictionary keys\n",
    "for sample in mean_melt_bulk_unj.keys(): \n",
    "    # Initialise blank timepoint-spanning lists within the output dictionaries\n",
    "    means_list = [\n",
    "        np.array(mean_melt_bulk_unj[sample][repeat]) \n",
    "        for repeat in list(mean_melt_bulk_unj[sample].keys())\n",
    "    ]\n",
    "    mean_melt_bulk_comb[sample] = np.mean(means_list, axis = 0)\n",
    "    std_melt_bulk_comb[sample] = np.std(means_list, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52850ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure S8b -- melting profiles and T_M calculation\n",
    "temp = np.array(list(range(25, 76)))\n",
    "colours = {'NS_A' : 'orangered', 'NS_B' : 'cyan', 'NS_C' : 'gray'}\n",
    "labels = {'NS_A' : 'A', 'NS_B' : 'B', 'NS_C' : 'C'}\n",
    "\n",
    "plt.subplots(3, 1, figsize = (3, 9), sharey = True)\n",
    "plt.subplots_adjust(hspace= .1)\n",
    "for sample, ind in zip(mean_melt_bulk_comb.keys(), range(len(mean_melt_bulk_comb.keys()))): \n",
    "    plt.subplot(3, 1, ind+1)\n",
    "    plt.plot(temp, \n",
    "             np.nan_to_num(mean_melt_bulk_comb[sample]), \n",
    "             lw = 2.0, label = labels[sample], color = colours[sample])\n",
    "    plt.fill_between(temp, \n",
    "                     np.nan_to_num(mean_melt_bulk_comb[sample]) - np.nan_to_num(std_melt_bulk_comb[sample]), \n",
    "                     np.nan_to_num(mean_melt_bulk_comb[sample]) + np.nan_to_num(std_melt_bulk_comb[sample]), \n",
    "                     color = colours[sample], alpha = 0.2)\n",
    "    ax = plt.gca()\n",
    "    ### DISPLAY HORIZONTAL THRESHOLD USED TO CALCULATE T_m\n",
    "    midpoint = (np.max(np.nan_to_num(mean_melt_bulk_comb[sample])) + np.min(np.nan_to_num(mean_melt_bulk_comb[sample])))/2\n",
    "    plt.axhline(midpoint, color = 'black', linestyle = 'dotted')\n",
    "    # DISPLAY VERTICAL LINE AT FOUND T_m VALUE\n",
    "    plt.axvline(temp[get_step(sigmoidify(np.nan_to_num(mean_melt_bulk_comb[sample])))], \n",
    "                lw = 2.0, linestyle = 'dotted', color = 'black')\n",
    "    plt.text(midpoint, \n",
    "             temp[get_step(sigmoidify(np.nan_to_num(mean_melt_bulk_comb[sample])))], \n",
    "             str(temp[get_step(sigmoidify(np.nan_to_num(mean_melt_bulk_comb[sample])))]) + r' °C', \n",
    "             fontsize = 20, color = colours[sample], \n",
    "             path_effects=[pe.withStroke(linewidth=0.8, foreground=\"black\")])\n",
    "    #print(r'$T_{m} = $', temp[get_step(sigmoidify(np.nan_to_num(mean_melt_bulk_comb[sample])))], '$^o$C')\n",
    "    ax.tick_params(direction = 'in', length = 6)\n",
    "    plt.ylabel(r'$\\rm\\mu_{CLD}$ [$\\rm\\mu$m]', fontsize = 20)\n",
    "    plt.yticks([0, 25, 50, 75, 100, 125, 150], [0, '', 50, '', 100, '', 150], fontsize = 20)\n",
    "    plt.ylim([-10, 160])\n",
    "    if ind == 2: \n",
    "        plt.xlabel('Temperature [°C]', fontsize = 20)\n",
    "        plt.xticks([25, 50, 75], [25, 50, 75], fontsize = 20)\n",
    "    else: \n",
    "        plt.xticks([25, 50, 75], [])\n",
    "    plt.legend(frameon = False, fontsize = 20, loc = 'upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1c - scatter plot of T_Ms (melting temperatures)\n",
    "# Source: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/broken_axis.html\n",
    "\n",
    "samples = ['A', 'B', 'C']\n",
    "mapping = {'A' : 'NS_A', 'B' : 'NS_B', 'C' : 'NS_C'}\n",
    "melting_temps = [\n",
    "    temp[get_step(sigmoidify(np.nan_to_num(mean_melt_bulk_comb[mapping[sample]])))] \n",
    "    for sample in samples\n",
    "]\n",
    "colors = ['orangered', 'cyan', 'silver']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize = (4, 4), gridspec_kw={'height_ratios': [4, 1]})\n",
    "fig.subplots_adjust(hspace=0.05)  # adjust space between Axes\n",
    "\n",
    "# plot the same data on both axes\n",
    "for (sample, melting_temp), color in zip(zip(samples, melting_temps), colors): \n",
    "    ax1.plot(sample, melting_temp, color=color, marker='o', ms = 12, mec='black')\n",
    "    ax2.plot(sample, melting_temp, color=color, marker='o', ms = 12, mec='black')\n",
    "\n",
    "# limit the view to different portions of the data\n",
    "ax1.set_ylim(35, 55)  # outliers only\n",
    "ax2.set_ylim(0, 5)  # most of the data\n",
    "\n",
    "# hide the spines between ax and ax2\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "# Make slanted lines to indicate discontinuous y-axis\n",
    "d = .5  # proportion of vertical to horizontal extent of the slanted line\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax1.tick_params(direction = 'in', length = 6)\n",
    "ax2.tick_params(direction = 'in', length = 6)\n",
    "ax2.set_xticklabels(['A', 'B', 'C'], fontsize = 25); \n",
    "ax1.set_yticks([40, 45, 50])\n",
    "ax1.set_yticklabels([40, 45, 50], fontsize = 25); \n",
    "ax2.set_yticks([0])\n",
    "ax2.set_yticklabels([0], fontsize = 25); \n",
    "ax1.set_xlim([-0.25, 2.25])\n",
    "ax2.set_xlabel('RNA Construct', fontsize = 25)\n",
    "ax1.set_ylabel(r'T$\\rm_M$ [°C]', fontsize = 25);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeae3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
